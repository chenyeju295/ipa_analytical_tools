#!/usr/bin/env python3
"""
IPA åˆ†æå·¥å…· - å¢å¼ºç‰ˆä¸»ç¨‹åº
æ”¯æŒå•ä¸ªæˆ–å¤šä¸ªIPAæ–‡ä»¶åˆ†æï¼Œé›†æˆç›¸ä¼¼æ€§åˆ†æåŠŸèƒ½
"""

import argparse
import sys
import os
import glob
from pathlib import Path
from typing import List, Dict
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'tools'))

from ipa_parser import IPAParser
from string_extractor import StringExtractor
from resource_analyzer import ResourceAnalyzer
from reporter import Reporter
from similarity_analyzer import SimilarityAnalyzer
from analyze_ipa_similarity import IPASimilarityAnalyzer


class BatchIPAAnalyzer:
    """æ‰¹é‡IPAåˆ†æå™¨"""
    
    def __init__(self, output_dir: str = "data/analysis_reports", verbose: bool = False):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.verbose = verbose
        self.results = []
        
    def analyze_single_ipa(self, ipa_path: str, min_string_length: int = 4) -> Dict:
        """åˆ†æå•ä¸ªIPAæ–‡ä»¶"""
        if self.verbose:
            print(f"ğŸ“± æ­£åœ¨åˆ†æ: {ipa_path}")
        
        try:
            # 1. è§£æIPAæ–‡ä»¶
            parser = IPAParser(ipa_path)
            parser.extract()
            app_info = parser.get_app_info()
            
            # 2. æå–å­—ç¬¦ä¸²
            string_extractor = StringExtractor(min_length=min_string_length)
            binary_path = parser.get_binary_path()
            strings_data = string_extractor.analyze(binary_path)
            
            # 3. åˆ†æèµ„æº
            resource_analyzer = ResourceAnalyzer()
            resources_data = resource_analyzer.analyze(parser.temp_dir)
            
            # 4. åˆå¹¶åˆ†æç»“æœ
            analysis_result = {
                'app_info': app_info,
                'strings': strings_data,
                'resources': resources_data,
                'analysis_meta': {
                    'tool_version': '2.0.0',
                    'source_file': ipa_path,
                    'min_string_length': min_string_length,
                    'analyzed_at': datetime.now().isoformat()
                }
            }
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            app_name = app_info.get('name', 'unknown_app')
            json_file = self.output_dir / f"{app_name}_analysis.json"
            
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            parser.cleanup()
            
            result = {
                'ipa_path': ipa_path,
                'app_name': app_name,
                'status': 'success',
                'output_file': str(json_file),
                'analysis_result': analysis_result
            }
            
            if self.verbose:
                print(f"âœ… å®Œæˆåˆ†æ: {app_name}")
            
            return result
            
        except Exception as e:
            error_result = {
                'ipa_path': ipa_path,
                'status': 'error',
                'error': str(e)
            }
            if self.verbose:
                print(f"âŒ åˆ†æå¤±è´¥: {ipa_path} - {e}")
            return error_result
    
    def analyze_multiple_ipas(self, ipa_paths: List[str], min_string_length: int = 4, max_workers: int = 3) -> List[Dict]:
        """å¹¶è¡Œåˆ†æå¤šä¸ªIPAæ–‡ä»¶"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ {len(ipa_paths)} ä¸ªIPAæ–‡ä»¶")
        
        results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_ipa = {
                executor.submit(self.analyze_single_ipa, ipa_path, min_string_length): ipa_path 
                for ipa_path in ipa_paths
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_ipa):
                ipa_path = future_to_ipa[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1
                    print(f"è¿›åº¦: {completed}/{len(ipa_paths)} - {ipa_path}")
                except Exception as e:
                    error_result = {
                        'ipa_path': ipa_path,
                        'status': 'error',
                        'error': str(e)
                    }
                    results.append(error_result)
                    print(f"âŒ å¤„ç†å¤±è´¥: {ipa_path} - {e}")
        
        self.results = results
        return results
    
    def print_batch_summary(self, results: List[Dict]):
        """æ‰“å°æ‰¹é‡åˆ†ææ‘˜è¦"""
        successful = [r for r in results if r['status'] == 'success']
        failed = [r for r in results if r['status'] == 'error']
        
        print("\n" + "=" * 60)
        print("æ‰¹é‡åˆ†ææ‘˜è¦")
        print("=" * 60)
        print(f"æ€»è®¡: {len(results)} ä¸ªæ–‡ä»¶")
        print(f"æˆåŠŸ: {len(successful)} ä¸ª")
        print(f"å¤±è´¥: {len(failed)} ä¸ª")
        
        if successful:
            print(f"\nâœ… æˆåŠŸåˆ†æçš„åº”ç”¨:")
            for result in successful:
                app_name = result.get('app_name', 'unknown')
                print(f"   â€¢ {app_name}")
        
        if failed:
            print(f"\nâŒ å¤±è´¥çš„æ–‡ä»¶:")
            for result in failed:
                print(f"   â€¢ {result['ipa_path']}: {result['error']}")
        
        print("=" * 60)
    
    def generate_similarity_analysis(self, target_apps: List[str] = None):
        """ç”Ÿæˆç›¸ä¼¼æ€§åˆ†æ"""
        print("\nğŸ” å¼€å§‹ç›¸ä¼¼æ€§åˆ†æ...")
        
        # ä½¿ç”¨å¢å¼ºçš„ç›¸ä¼¼æ€§åˆ†æå™¨ï¼Œé»˜è®¤å¯ç”¨æ™ºèƒ½è¿‡æ»¤
        similarity_analyzer = SimilarityAnalyzer(
            analysis_dir=str(self.output_dir), 
            filter_common_words=True,
            target_apps=target_apps
        )
        comprehensive_report = similarity_analyzer.generate_comprehensive_report()
        
        # æ‰“å°æ‘˜è¦
        similarity_analyzer.print_similarity_summary(comprehensive_report)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.output_dir / "comprehensive_similarity_analysis.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š è¯¦ç»†ç›¸ä¼¼æ€§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # å…¼å®¹åŸæœ‰çš„ç›¸ä¼¼æ€§åˆ†æå·¥å…·ï¼ˆå¦‚æœæ•°æ®åº“ç»“æ„åŒ¹é…ï¼‰
        try:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ—§ç‰ˆæ•°æ®åº“ç»“æ„
            import sqlite3
            db_path = self.output_dir.parent / "data" / "word_library.db"
            if db_path.exists():
                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()
                # æ£€æŸ¥æ˜¯å¦æœ‰apps_liståˆ—
                cursor.execute("PRAGMA table_info(words)")
                columns = [row[1] for row in cursor.fetchall()]
                conn.close()
                
                if 'apps_list' in columns:
                    legacy_analyzer = IPASimilarityAnalyzer()
                    legacy_analyzer.analyze_all()
                else:
                    print("â„¹ï¸  æ•°æ®åº“ç»“æ„å·²æ›´æ–°ï¼Œä½¿ç”¨æ–°ç‰ˆç›¸ä¼¼æ€§åˆ†æå™¨")
            else:
                print("â„¹ï¸  æœªæ‰¾åˆ°ä¼ ç»Ÿæ•°æ®åº“ï¼Œä½¿ç”¨æ–°ç‰ˆç›¸ä¼¼æ€§åˆ†æå™¨")
        except Exception as e:
            print(f"â„¹ï¸  ä¼ ç»Ÿç›¸ä¼¼æ€§åˆ†æä¸å¯ç”¨: {e}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='IPAæ–‡ä»¶åˆ†æå·¥å…· - å¢å¼ºç‰ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  # å•ä¸ªæ–‡ä»¶åˆ†æ
  python main_enhanced.py app.ipa
  
  # å¤šä¸ªæ–‡ä»¶åˆ†æ
  python main_enhanced.py app1.ipa app2.ipa app3.ipa
  
  # ä½¿ç”¨é€šé…ç¬¦
  python main_enhanced.py ipas/*.ipa
  
  # æŒ‡å®šç›®å½•ä¸­çš„æ‰€æœ‰IPAæ–‡ä»¶
  python main_enhanced.py --directory ipas/
  
  # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šå¹¶æ‰§è¡Œç›¸ä¼¼æ€§åˆ†æ
  python main_enhanced.py ipas/*.ipa --similarity --verbose
        '''
    )
    
    parser.add_argument('ipa_files', nargs='*', help='IPAæ–‡ä»¶è·¯å¾„(æ”¯æŒå¤šä¸ª)')
    parser.add_argument('-d', '--directory', help='åŒ…å«IPAæ–‡ä»¶çš„ç›®å½•')
    parser.add_argument('-o', '--output', default='data/analysis_reports', help='è¾“å‡ºç›®å½• (é»˜è®¤: data/analysis_reports)')
    parser.add_argument('--similarity', action='store_true', help='æ‰§è¡Œç›¸ä¼¼æ€§åˆ†æ')
    parser.add_argument('--csv', action='store_true', help='ç”ŸæˆCSVæ ¼å¼æŠ¥å‘Š')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--min-string-length', type=int, default=4, help='æœ€å°å­—ç¬¦ä¸²é•¿åº¦ (é»˜è®¤: 4)')
    parser.add_argument('--max-workers', type=int, default=3, help='å¹¶è¡Œå¤„ç†çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 3)')
    
    return parser.parse_args()


def collect_ipa_files(args) -> List[str]:
    """æ”¶é›†è¦åˆ†æçš„IPAæ–‡ä»¶"""
    ipa_files = []
    
    # ä»å‘½ä»¤è¡Œå‚æ•°æ”¶é›†æ–‡ä»¶
    if args.ipa_files:
        for pattern in args.ipa_files:
            if '*' in pattern or '?' in pattern:
                # é€šé…ç¬¦æ¨¡å¼
                matched_files = glob.glob(pattern)
                ipa_files.extend(matched_files)
            else:
                # ç›´æ¥æ–‡ä»¶è·¯å¾„
                ipa_files.append(pattern)
    
    # ä»ç›®å½•æ”¶é›†æ–‡ä»¶
    if args.directory:
        directory = Path(args.directory)
        if directory.exists() and directory.is_dir():
            ipa_files.extend(str(f) for f in directory.glob('*.ipa'))
        else:
            print(f"é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {args.directory}")
            sys.exit(1)
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§
    valid_files = []
    for file_path in ipa_files:
        if os.path.exists(file_path):
            valid_files.append(file_path)
        else:
            print(f"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path}")
    
    return valid_files


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # æ”¶é›†IPAæ–‡ä»¶
    ipa_files = collect_ipa_files(args)
    
    if not ipa_files:
        print("é”™è¯¯: æœªæ‰¾åˆ°è¦åˆ†æçš„IPAæ–‡ä»¶")
        print("è¯·æŒ‡å®šIPAæ–‡ä»¶è·¯å¾„æˆ–ä½¿ç”¨ --directory æŒ‡å®šåŒ…å«IPAæ–‡ä»¶çš„ç›®å½•")
        sys.exit(1)
    
    # å»é‡å¹¶æ’åº
    ipa_files = sorted(list(set(ipa_files)))
    
    print(f"æ‰¾åˆ° {len(ipa_files)} ä¸ªIPAæ–‡ä»¶å¾…åˆ†æ")
    if args.verbose:
        for i, file_path in enumerate(ipa_files, 1):
            print(f"  {i}. {file_path}")
    
    # åˆ›å»ºæ‰¹é‡åˆ†æå™¨
    analyzer = BatchIPAAnalyzer(output_dir=args.output, verbose=args.verbose)
    
    try:
        if len(ipa_files) == 1:
            # å•æ–‡ä»¶åˆ†æ
            result = analyzer.analyze_single_ipa(ipa_files[0], args.min_string_length)
            
            if result['status'] == 'success':
                # æ‰“å°å•æ–‡ä»¶æ‘˜è¦
                reporter = Reporter(result['analysis_result'])
                reporter.print_summary()
                
                # ç”ŸæˆCSVæŠ¥å‘Šï¼ˆå¦‚æœéœ€è¦ï¼‰
                if args.csv:
                    app_name = result['app_name']
                    csv_file = Path(args.output) / f"{app_name}_analysis.csv"
                    reporter.generate_csv(str(csv_file))
                    print(f"CSVæŠ¥å‘Šå·²ä¿å­˜åˆ°: {csv_file}")
                
                print(f"JSONæŠ¥å‘Šå·²ä¿å­˜åˆ°: {result['output_file']}")
            else:
                print(f"åˆ†æå¤±è´¥: {result['error']}")
                sys.exit(1)
        else:
            # å¤šæ–‡ä»¶åˆ†æ
            results = analyzer.analyze_multiple_ipas(
                ipa_files, 
                args.min_string_length, 
                args.max_workers
            )
            
            # æ‰“å°æ‰¹é‡æ‘˜è¦
            analyzer.print_batch_summary(results)
            
            # ç”ŸæˆCSVæŠ¥å‘Šï¼ˆå¦‚æœéœ€è¦ï¼‰
            if args.csv:
                successful_results = [r for r in results if r['status'] == 'success']
                for result in successful_results:
                    try:
                        reporter = Reporter(result['analysis_result'])
                        app_name = result['app_name']
                        csv_file = Path(args.output) / f"{app_name}_analysis.csv"
                        reporter.generate_csv(str(csv_file))
                        if args.verbose:
                            print(f"CSVæŠ¥å‘Šå·²ç”Ÿæˆ: {csv_file}")
                    except Exception as e:
                        print(f"ç”ŸæˆCSVæŠ¥å‘Šå¤±è´¥ {result['app_name']}: {e}")
        
        # æ‰§è¡Œç›¸ä¼¼æ€§åˆ†æ
        if args.similarity:
            # æ”¶é›†å½“å‰æ‰¹æ¬¡æˆåŠŸåˆ†æçš„åº”ç”¨åç§°
            if len(ipa_files) == 1:
                # å•æ–‡ä»¶åˆ†æ
                if result['status'] == 'success':
                    target_apps = [result['app_name']]
                else:
                    target_apps = None
            else:
                # å¤šæ–‡ä»¶åˆ†æ
                target_apps = [r['app_name'] for r in results if r['status'] == 'success']
            
            analyzer.generate_similarity_analysis(target_apps)
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­äº†åˆ†æè¿‡ç¨‹")
        sys.exit(1)
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)
    
    print("\nğŸ‰ æ‰€æœ‰åˆ†æä»»åŠ¡å®Œæˆï¼")


if __name__ == '__main__':
    main() 